{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "752d1405-cd81-490f-9167-3cf84bb90287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import *\n",
    "\n",
    "#TODO actually load real weights from a generated model\n",
    "class ALSModel(nn.Module):\n",
    "    def __init__(self, n_items: int, n_factors: int):\n",
    "        super(ALSModel, self).__init__()\n",
    "        #TODO add user --> item\n",
    "        self.emb = nn.Embedding(n_items, n_factors)\n",
    "        self.it2ind = {str(i): i for i in range(n_items)}# TODO use real values\n",
    "        self.ind2it = {v:k for k,v in self.it2ind.items()}\n",
    "        \n",
    "    \n",
    "    def map_out(self, scores: torch.Tensor, indices: torch.Tensor) -> Dict[str, float]:\n",
    "        inds: List[int] = indices.squeeze().tolist()\n",
    "        scorez: List[float] = scores.squeeze().tolist()\n",
    "        return {self.ind2it[i]: s for i, s in zip(inds, scorez)}\n",
    "    \n",
    "    #def forward(self, ind: torch.Tensor, size: int = 30)-> torch.Tensor:#  -> Dict[str, float]:\n",
    "    def forward(self, ind: torch.Tensor)-> torch.Tensor:\n",
    "    #def forward(self, ind: torch.Tensor, size: int = 5)-> torch.Tensor:\n",
    "        #ind = torch.tensor([self.it2ind[inp]]).long()\n",
    "        u = self.emb(ind)\n",
    "        scores = u @ self.emb.weight.t()\n",
    "        s, i = scores.topk(5)\n",
    "        #return self.map_out(s, i)\n",
    "        return s.squeeze()\n",
    "        \n",
    "        \n",
    "n_items, n_factors = (10000, 128)     \n",
    "m = ALSModel(n_items, n_factors)\n",
    "m2 = torch.jit.script(m)\n",
    "    \n",
    "#TODO can we actually map strings and stuff in ONNX? It seems like we cant??? We cant even send in ints to modify functions.\n",
    "#TODO export model to ONNX\n",
    "#m(\"5\", size=5), m2(\"5\", size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf787bc-82ec-4c17-beff-8714bc898988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([107.4750,  46.6246,  41.8947,  38.1183,  36.3196],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2(torch.ones(1).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "197e4cde-b0b0-4d2a-b95a-50543a6d98b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%ind.1 : Long(1, strides=[1], requires_grad=0, device=cpu),\n",
      "      %emb.weight : Float(10000, 128, strides=[128, 1], requires_grad=0, device=cpu),\n",
      "      %8 : Float(128, 10000, strides=[1, 128], requires_grad=0, device=cpu)):\n",
      "  %u : Float(1, 128, strides=[128, 1], device=cpu) = onnx::Gather(%emb.weight, %ind.1) # /home/n651042/micromamba/envs/attention/lib/python3.10/site-packages/torch/nn/functional.py:2183:11\n",
      "  %scores : Float(1, 10000, strides=[10000, 1], device=cpu) = onnx::MatMul(%u, %8) # /tmp/ipykernel_12857/3742737406.py:26:17\n",
      "  %s : Float(1, 5, strides=[5, 1], device=cpu), %i : Long(1, 5, strides=[5, 1], device=cpu) = onnx::TopK[axis=-1, k=5](%scores) # /tmp/ipykernel_12857/3742737406.py:27:15\n",
      "  %7 : Float(5, strides=[1], requires_grad=1, device=cpu) = onnx::Squeeze(%s) # /tmp/ipykernel_12857/3742737406.py:29:15\n",
      "  return (%7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = (torch.ones(1).long())\n",
    "#x = (torch.ones(1).long(), 3)\n",
    "file_path = \"data/model.onnx\"\n",
    "torch.onnx.export(m2, x, file_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50bc556b-08d6-44d4-a6a3-c46e09c582b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "# ensure a valid onnx file\n",
    "onnx.checker.check_model(onnx.load(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09324e29-97b6-4a71-8598-1fac0ea6bf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([149.96411 ,  44.02415 ,  43.7658  ,  41.755444,  41.204   ],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# sanity test outputs\n",
    "ort_session = onnxruntime.InferenceSession(file_path)\n",
    "inp = np.ones(1, dtype=np.int64) * 99\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: inp}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "print(ort_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162f4db-fd25-4912-acb2-0733445f0323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
