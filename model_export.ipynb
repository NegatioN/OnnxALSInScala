{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "843f03bf-0353-4822-9ab8-4060bee7637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import *\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "def make_mapping_node(amap, input_name, output_name, keys='strings', values='int64s'):\n",
    "    valid_key_values = ['strings', 'int64s', 'floats']\n",
    "    assert keys in valid_key_values and values in valid_key_values, f'Keys or Values not in valid set of {valid_key_values}'\n",
    "    other_inps = {f'keys_{keys}': amap.keys(), f'values_{values}': amap.values()}\n",
    "    return onnx.helper.make_node('LabelEncoder', inputs=[input_name], outputs=[output_name], domain='ai.onnx.ml', **other_inps)\n",
    "    \n",
    "def replace_node_value(input_output, new_node_value, replace_node_name):\n",
    "    existing_node = list(filter(lambda x: x.name == replace_node_name, input_output))[0]\n",
    "    input_output.remove(existing_node)\n",
    "    input_output.append(new_node_value)\n",
    "    \n",
    "\n",
    "class MLTModel(nn.Module):\n",
    "    def __init__(self, it2ind_mapping: Dict[str, int], vectors):\n",
    "        super(MLTModel, self).__init__()\n",
    "        n_items = len(vectors)\n",
    "        n_factors = len(vectors[0])\n",
    "        self.emb = nn.Embedding(n_items, n_factors, _weight=torch.from_numpy(vectors))\n",
    "        self.it2ind = it2ind_mapping\n",
    "        self.ind2it = {v:k for k,v in self.it2ind.items()}\n",
    "    \n",
    "    # model forward pass\n",
    "    def forward(self, ind: torch.Tensor, size: int = 5)-> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        u = self.emb(ind)\n",
    "        scores = u @ self.emb.weight.t()\n",
    "        s, i = scores.topk(size)\n",
    "        return s.squeeze(), i.squeeze()\n",
    "    \n",
    "    # most of the export code is here to map strings in and out of the network\n",
    "    # which is not supported by default when exporting a PyTorch graph --> ONNX\n",
    "    def add_onnx_mappings(self, path, op_set):\n",
    "        import onnx\n",
    "        om = onnx.load(path)\n",
    "        \n",
    "        #TODO obviously simplify this, and possibly make it not neccessary to think about for the modeller.\n",
    "        # Map input\n",
    "        in_node_value = onnx.helper.make_tensor_value_info('contentId', onnx.TensorProto.STRING, [None])\n",
    "        replace_node_name = 'contentId_ind'\n",
    "        n = make_mapping_node(self.it2ind, in_node_value.name, replace_node_name)\n",
    "        om.graph.node.insert(0, n)\n",
    "        replace_node_value(om.graph.input, in_node_value, replace_node_name)\n",
    "        \n",
    "        # Map output\n",
    "        out_node_value = onnx.helper.make_tensor_value_info('contentIdd', onnx.TensorProto.STRING, [None])\n",
    "        replace_node_name = 'indices'\n",
    "        n = make_mapping_node(self.ind2it, replace_node_name, out_node_value.name, keys='int64s', values='strings')\n",
    "        om.graph.node.append(n)\n",
    "        replace_node_value(om.graph.output, out_node_value, replace_node_name)\n",
    "        \n",
    "        # finalize model\n",
    "        model = onnx.helper.make_model(om.graph, opset_imports=[onnx.helper.make_opsetid('ai.onnx.ml', 2), onnx.helper.make_opsetid('', op_set)])\n",
    "        onnx.checker.check_model(model)\n",
    "        onnx.save(model, path)\n",
    "        \n",
    "    \n",
    "    def export(self, path='model.onnx', onnx_op_set=16):\n",
    "        input_names = ['contentId_ind', 'size']\n",
    "        output_names = ['scores', 'indices'] # this should be a convention? \n",
    "        # Dynamic axes does nothing since we re-export the model atm.\n",
    "        dynamic_axes = {name: [0] for name in output_names}\n",
    "        jit_model = torch.jit.script(self)\n",
    "        dummy_input = (torch.ones(1).long(), 3)\n",
    "        dummy_input = tuple(1 for _ in input_names)\n",
    "        torch.onnx.export(jit_model, dummy_input, path, input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axes, verbose=True, opset_version=onnx_op_set)\n",
    "        self.add_onnx_mappings(path, onnx_op_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944d2f4-9e16-411d-ae78-28c35c1176b4",
   "metadata": {},
   "source": [
    "# Use some real data\n",
    "Pull down a model, and import the newline-delimited json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "97610722-12a9-4d19-96ac-7e759ae3cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/model.json', 'r') as f:\n",
    "    rows = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d0944c43-3e9f-497a-b793-bdfa0c7baa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_model_data = {x['contentId']: x['factors'] for x in rows}\n",
    "\n",
    "vectors = np.array([x for x in real_model_data.values()])\n",
    "name2ind = {n:i for i, n in enumerate(real_model_data.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d67d03b5-ba3d-4d0a-bdc4-41f31935a45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%contentId_ind : Long(requires_grad=0, device=cpu),\n",
      "      %size : Long(device=cpu),\n",
      "      %emb.weight : Double(9382, 20, strides=[20, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_12 : Double(20, 9382, strides=[1, 20], requires_grad=0, device=cpu)):\n",
      "  %u : Double(20, strides=[1], device=cpu) = onnx::Gather[onnx_name=\"Gather_0\"](%emb.weight, %contentId_ind) # /home/n651042/micromamba/envs/onnx/lib/python3.10/site-packages/torch/nn/functional.py:2199:11\n",
      "  %scores.1 : Double(9382, strides=[1], device=cpu) = onnx::MatMul[onnx_name=\"MatMul_1\"](%u, %onnx::MatMul_12) # /tmp/ipykernel_5861/4285441363.py:33:17\n",
      "  %onnx::Reshape_6 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={1}, onnx_name=\"Constant_2\"]() # /tmp/ipykernel_5861/4285441363.py:34:15\n",
      "  %onnx::TopK_7 : Long(1, strides=[1], device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"Reshape_3\"](%size, %onnx::Reshape_6) # /tmp/ipykernel_5861/4285441363.py:34:15\n",
      "  %s : Double(*, device=cpu), %i : Long(*, device=cpu) = onnx::TopK[axis=-1, largest=1, sorted=1, onnx_name=\"TopK_4\"](%scores.1, %onnx::TopK_7) # /tmp/ipykernel_5861/4285441363.py:34:15\n",
      "  %scores : Double(requires_grad=1, device=cpu) = onnx::Squeeze[onnx_name=\"Squeeze_5\"](%s) # /tmp/ipykernel_5861/4285441363.py:35:15\n",
      "  %indices : Long(requires_grad=0, device=cpu) = onnx::Squeeze[onnx_name=\"Squeeze_6\"](%i) # /tmp/ipykernel_5861/4285441363.py:35:28\n",
      "  return (%scores, %indices)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MLTModel(name2ind, vectors)\n",
    "model.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f83ce439-3fb4-438b-989f-65d62eef2d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just make sure all or ducks are in a row\n",
    "np.isclose(vectors[0], np.array(rows[0]['factors'])).all() , np.isclose(model.emb.weight[0].detach().numpy(), np.array(rows[0]['factors'])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "61663614-50d5-4c80-95e6-211a91a33652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 13:37:35.779964309 [W:onnxruntime:, execution_frame.cc:812 VerifyOutputSizes] Expected shape from model of {} does not match actual shape of {10} for output scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ski-vm-junior-og-u23': 1.0000000000000002,\n",
       " 'dama-til': 0.999999999638621,\n",
       " 'KOIF43007811': 0.9999999995384318,\n",
       " 'KMNO10008822': 0.9999999992626512,\n",
       " 'KOIF75000417': 0.9999999976467383,\n",
       " 'friidrett-nm': 0.9999999963539263,\n",
       " 'verdens-beste-landslag': 0.9999999922260039,\n",
       " 'kriger': 0.9999999904107656,\n",
       " 'lunsj': 0.9999999903992889,\n",
       " 'der-ingen-skulle-tru-at-nokon-kunne-bu': 0.9999999876724911}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession('model.onnx')\n",
    "inp = np.array([\"ski-vm-junior-og-u23\"])\n",
    "ort_inputs = {'contentId': inp, 'size': np.ones(1, dtype=np.int64) * 10}\n",
    "res = ort_session.run(None, ort_inputs)\n",
    "{n:s for n, s in zip(res[1], res[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "593857cc-b4d5-4087-8eed-3c8c65824561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'', 'LabelEncoder'},\n",
       " {'Gather', 'Gather_0'},\n",
       " {'MatMul', 'MatMul_1'},\n",
       " {'Constant', 'Constant_2'},\n",
       " {'Reshape', 'Reshape_3'},\n",
       " {'TopK', 'TopK_4'},\n",
       " {'Squeeze', 'Squeeze_5'},\n",
       " {'Squeeze', 'Squeeze_6'},\n",
       " {'', 'LabelEncoder'}]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "om = onnx.load('model.onnx')\n",
    "list(map(lambda x: {x.name, x.op_type}, om.graph.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "447aeb77-42c4-46ca-9bdf-ea0afe649818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add user --> item\n",
    "#TODO add dynamic_axes to final output model, to stop warning\n",
    "#TODO meta level error handling. etc, having size>num_items_in_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff4a77-b9a6-425a-9900-b116b4f09660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
